## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
datatable(df)
str_count(corp[1]$content)
sum(df$freq)
df<- sort(rowSums(tdm),decreasing=TRUE)
df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
datatable(df)
tmp
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp
tmp <- str_replace_all(tmp, pattern="\\=", replacement = "")
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
#corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
dtm <- as.matrix(DocumentTermMatrix(corp))
View(df)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
dtm <- as.matrix(DocumentTermMatrix(corp))
View(df)
View(dtm)
df<- sort(rowSums(dtm),decreasing=TRUE)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
tdm <- as.matrix(TermDocumentMatrix(corp))
df<- sort(rowSums(tdm),decreasing=TRUE)
df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
datatable(df)
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('DT')
install.packages("DT")
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('DT')
install.packages("DT")
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('DT')
install.packages("DT")
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('RWeka')
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('RWeka')
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('RWeka')
install.packages("RWeka")
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('RWeka')
xgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
tdmatrix = TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
inspect(tdmatrix)
xgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdmatrix = TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
tdmatrix
inspect(tdmatrix)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdm = TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
#tdm <- as.matrix(TermDocumentMatrix(corp))
df<- sort(rowSums(tdm),decreasing=TRUE)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
## create all as function that either does tokenizer or does not depending on function call
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
#tdm <- as.matrix(TermDocumentMatrix(corp))
inspect(tdm)
temp <- inspect(tdm)
FreqMat <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
View(FreqMat)
FreqMat <- slam::row_sums(tdm)
FreqMat
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df
df
df <- data.frame(word=names(df), freq=df)
View(df)
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 4))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
View(df)
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 4))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
View(df)
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
View(df)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
## create all as function that either does tokenizer or does not depending on function call
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
View(df)
tmp <- readLines("assign6.sample.txt")
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
## create all as function that either does tokenizer or does not depending on function call
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
tdm
inspect(tdm)
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('DT')) install.packages('DT')
if (!require('RWeka')) install.packages('RWeka')
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
## create all as function that either does tokenizer or does not depending on function call
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
df<- sort(rowSums(tdm),decreasing=TRUE)
df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
oneword <- wordprob('assign6.sample.txt')
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
## create all as function that either does tokenizer or does not depending on function call
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
oneword <- wordprob('assign6.sample.txt')
datatable(oneword)
oneword <- wordprob('assign6.sample.txt')
datatable(oneword)
twowords <- wordprob('assign6.sample.txt', ngrams=2)
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
tdm
ngrams=2
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
View(df)
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
corp <- Corpus(VectorSource(docname))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
oneword <- wordprob('assign6.sample.txt')
datatable(oneword)
twowords <- wordprob('assign6.sample.txt', ngrams=2)
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
oneword <- wordprob('assign6.sample.txt')
datatable(oneword)
twowords <- wordprob('assign6.sample.txt', ngrams=2)
docname <- ''assign6.sample.txt''
docname <- 'assign6.sample.txt'
View(oneword)
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = "")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp
corp[1]
corp[1]$content
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
}tmp
tmp
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp[1]$content
corp <- tm_map(corp, removePunctuation)
corp[1]$content
corp <- tm_map(corp, removeNumbers)
corp[1]$content
tmp <- readLines(docname)
#    tmp <- str_c(tmp, collapse = " ")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stop
corp[1]$content
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
?removePunctuation
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = "")
#    tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#    tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
corp[1]$content
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = ngrams))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
View(df)
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('RWeka')) install.packages('RWeka')
if (!require('DT')) install.packages('DT')
install.packages("rJava",type='source')
if (!require('stringr')) install.packages('stringr')
if (!require('tm')) install.packages('tm')
if (!require('SnowballC')) install.packages('SnowballC')
if (!require('RTextTools')) install.packages('RTextTools')
if (!require('RWeka')) install.packages('RWeka')
if (!require('DT')) install.packages('DT')
install.packages('RWeka')
wordprob <- function(docname, ngrams=1){
tmp <- readLines(docname)
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
tmp <- str_replace_all(tmp, pattern="\\=", replacement = " ")
corp <- Corpus(VectorSource(tmp))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
corp <- tm_map(corp, removeNumbers)
#corp <- tm_map(corp, removeWords, stopwords("english"))
xgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = xgramTokenizer))
df <- sort(slam::row_sums(tdm), decreasing = TRUE)
df <- data.frame(word=names(df), freq=df)
#tdm <- as.matrix(TermDocumentMatrix(corp))
#df<- sort(rowSums(tdm),decreasing=TRUE)
#df <- data.frame(word = names(df),freq=df)
df$probability <- round(df$freq / sum(df$freq), 5)
## option for total number of words in a file above
## option for total in freq df = sum(df$freq)
rownames(df) <- NULL
df
}
oneword <- wordprob('assign6.sample.txt')
datatable(oneword)
twowords <- wordprob('assign6.sample.txt', ngrams=2)
View(oneword)
