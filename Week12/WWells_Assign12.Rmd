---
title: "DATA 605: Fundamentals of Computational Mathematics"
author: "Walt Wells, Spring 2017"
subtitle: "Assignment 12"
output:
  html_document:
    css: ../custom.css
    highlight: zenburn
    theme: lumen
---
# Bias Variance Tradeoff in R

Using the stats and boot libraries in R perform a cross-validation experiment to observe the bias variance tradeoff. You'll use the auto data set from previous assignments. This dataset has 392 observations across 5 variables. We want to fit a polynomial model of various degrees using the glm function in R and then measure the cross validation error using cv.glm function.

Fit various polynomial models to compute mpg as a function of the other four variables acceleration, weight, horsepower, and displacement using glm function. For example:

```
glm.fit=glm(mpg~poly(disp+hp+wt+acc,2), data=auto)
cv.err5[2]=cv.glm(auto,glm.fit,K=5)$delta[1]
```

will fit a 2nd degree polynomial function between mpg and the remaining 4 variables and perform 5 iterations of cross-validations. This result will be stored in a cv.err5 array. cv.glm returns the estimated cross validation error and its adjusted value in a variable called delta. Please see the help on cv.glm to see more information.

Once you have fit the various polynomials from degree 1 to 8, you can plot the cross-validation error function as

```
degree=1:8
plot(degree,cv.err5,type='b')
```

Please create an R-markdown document where you load the auto data set, perform the polynomial fit and then plot the resulting 5 fold cross validation curve.  Your output should show the characteristic U-shape illustrating the tradeoff between bias and variance.

### Environment Prep

```{r}
library(stats); library(boot)
```

### Get Data

```{r}
url <- 'https://raw.githubusercontent.com/wwells/CUNY_DATA_605/master/Week05/auto-mpg.data'
auto <- read.table(url)
names(auto) <- c('displacement', 'horsepower', 'weight', 'acceleration', 'mpg')
```


### Poly Function

```{r}
# takes desired max polynomial degrees and number of cross validation sims
# fits model for each polynomial degree up to the max
# then creates a table of fit errors for number of cross validations sims
# we can later take the mean of these columns to understand bias/variance tradeoffs
polyfitter <- function(maxpoly, nsim){
    range <- seq(1,maxpoly)
    header <- ''
    cv.err <- ''
    for (i in range){
        # fit model for particular polynomial
        glm.fit <- glm(mpg~poly(displacement+horsepower+weight+acceleration,i), data=auto)
        x <- ''
        
        # do several cross validations
        for (n in 1:nsim) {
            x[n] <- as.numeric(cv.glm(auto, glm.fit, K=5)$delta[1])
        }
        x <- as.numeric(x)
        cv.err <- data.frame(cv.err, x)
        header <- c(header, paste0('Poly_', i))
    }
    cv.err <- cv.err[, -1]
    names(cv.err) <- header[-1]
    return(cv.err)
}
```

### Run function, get fit means

```{r}
# number of times cross validate each poly fit
n <- 100
# max degree of polynomial to model
p <- 11

Poly8 <- polyfitter(p, n)
m.Poly8 <- colMeans(Poly8)
```

### Plot

```{r}
title <- paste0('GLM Model Fit Means, ', n, ' Samples')

degrees <- 1:p
plot(degrees, m.Poly8, type='b', ylab='E_MSE', xlab='Polynomial Degree', main=title, col='blue')

```

### Summary

We can see that the best balance of bias/variance tradeoff for a GLM model is around Polynomial Degree 2 or 3.   First degree introduces too much bias, via underfit.   Later degrees overfit.  Creating a GLM model of polynomial degree 2 or 3 is all the complexity we need to model MPG in the Auto dataset.  

```{r}
m.Poly8
```

### References

* http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/
* https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf
